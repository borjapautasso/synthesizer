"""A module containing a pipeline helper class.

This module contains the `Pipeline` class, which is used to run observable
generation pipelines on a set of galaxies. To use this functionality the user
needs to define the properties of the Pipeline and a function to load the
galaxies. The user can then call the various methods to generate the mock
data they need, simplifying a complex pipeline full of boilerplate code to a
handfull of definitions and calls to the Pipeline object.

Example usage:
```python
    from synthesizer import Pipeline

    pipeline = Pipeline(
        gal_loader_func=load_galaxy,
        emission_model=emission_model,
        instruments=[instrument1, instrument2],
        n_galaxies=1000,
        nthreads=4,
        comm=None,
        verbose=1,
        )

    pipeline.load_galaxies()
    pipeline.get_spectra()
    pipeline.get_photometry_luminosities()
    pipeline.write("output.hdf5")
```
"""

import time

import numpy as np
from unyt import unyt_array

from synthesizer import check_openmp, exceptions
from synthesizer.instruments import Instrument, InstrumentCollection
from synthesizer.instruments.filters import FilterCollection
from synthesizer.pipeline.pipeline_io import PipelineIO
from synthesizer.pipeline.pipeline_utils import (
    combine_list_of_dicts,
    count_and_check_dict_recursive,
)
from synthesizer.synth_warnings import warn
from synthesizer.utils.art import Art


class Pipeline:
    """
    A class for running observable generation pipelines on a set of galaxies.

    To use this class the user must instantiate it with a galaxy loading
    function, an emission model defining the different emissions that will be
    included in the pipeline, any instruments that will be used to make
    observations, and the number of galaxies that will be loaded.

    Optionally the user can also specify the number of threads to use if
    Synthesizer has been installed with OpenMP support, and an MPI communicator
    if they are running over MPI.

    Finally the verbosity level can be set to control the amount of output.

    Once the Pipeline object has been instantiated the user can call the
    various methods to generate the data they need.

    For spectra:
        - get_spectra (passing a cosmology object if redshifted spectra are
            required)
        - get_lnu_data_cubes (resolved spectral data cubes)
        - get_fnu_data_cubes (resolved spectral data cubes)

    For photometry:
        - get_photometry_luminosities
        - get_photometry_fluxes

    For emission lines:
        - get_lines (passing a list of line IDs to generate)

    For images (with optional PSF and noise based on the instrument):
        - get_images_luminosity
        - get_images_flux

    For the SFZH grid:
        - get_sfzh (passing a Grid object)


    The user can also add their own analysis functions to the pipeline which
    will be run on each galaxy once all data has been generated. These
    functions should take a galaxy object as the first argument and can take
    any number of additional arguments and keyword arguments. The results of
    these functions should be attached to the galaxy object, either as base
    level attributes or dictionaries containing the computed values. These
    attributes should be unique to the function to avoid overwriting existing
    attributes (they should be named what is passed to the result_attribute
    argument, see add_analysis_func for more details).

    Finally the user can write out the data generated by the pipeline using the
    write method. This will write out the data to an HDF5 file.

    Attributes:
        emission_model (EmissionModel):
            The emission model to use for the pipeline.
        instruments (list):
            A list of Instrument objects to use for the pipeline.
        n_galaxies (int):
            How many galaxies will we load in total (i.e. not per rank if using
            MPI)?
        nthreads (int):
            The number of threads to use for shared memory parallelism.
            Default is 1.
        comm (MPI.Comm):
            The MPI communicator to use for MPI parallelism. Default is None.
        verbose (int):
            How talkative are we?
            0: No output beyond hello and goodbye.
            1: Outputs with timings but only on rank 0 (when using MPI).
            2: Outputs with timings on all ranks (when using MPI).
        galaxies (list):
            A list of Galaxy objects that have been loaded.
        filters (FilterCollection):
            A combined collection of all the filters from the instruments.
    """

    def __init__(
        self,
        emission_model,
        instruments=(),
        nthreads=1,
        comm=None,
        verbose=1,
    ):
        """
        Initialise the Pipeline object.

        This will not perform any part of the calculation, it only sets it up.

        This will attach all the passed attributes of the pipeline and set up
        anything we'll need later like MPI variables (if applicable), flags
        to indicate what stages we've completed and containers for any
        ouputs and additional analysis functions.

        This will also check arguments are sensible, e.g.
            - The galaxy loader function is callable and takes at least one
              argument (the galaxy index).
            - Synthesizer has been installed with OpenMP support if multiple
              threads are requested.

        Args:
            emission_model (EmissionModel): The emission model to use for the
                pipeline.
            instruments (Instrument, InstrumentCollection): Either a singular
                Instrument object, or an InstrumentCollection containing
                multiple Instrument objects.
            nthreads (int): The number of threads to use for shared memory
                parallelism. Default is 1.
            comm (MPI.Comm): The MPI communicator to use for MPI parallelism.
                Default is None.
            verbose (int): How talkative are we? 0: No output beyond hello and
                goodbye. 1: Outputs with timings but only on rank 0 (when using
                MPI). 2: Outputs with timings on all ranks (when using MPI).
        """
        # Attributes to track timing
        self._start_time = time.perf_counter()

        # Attach all the attributes we need to know what to do
        self.emission_model = emission_model

        # Check if it's a single instrument
        if isinstance(instruments, Instrument):
            # mock up an InstrumentCollection
            collection = InstrumentCollection()
            collection.add_instruments(self, instruments)
            self.instruments = collection
        else:
            self.instruments = instruments

        # Set verbosity
        self.verbose = verbose

        # How many galaxies are we going to be looking at?
        self.n_galaxies = 0
        self.n_galaxies_local = 0  # Only applicable when using MPI

        # Define the container to hold the galaxies
        self.galaxies = []

        # How many threads are we using for shared memory parallelism?
        self.nthreads = nthreads

        # Check if we can use OpenMP
        if self.nthreads > 1 and not check_openmp():
            raise exceptions.MissingPartition(
                "Can't use multiple threads without OpenMP support. "
                " Install with: `WITH_OPENMP=1 pip install .`"
            )

        # It's quicker for us to collect together all filters and apply them
        # in one go, so we collect them together here. Note that getting
        # photometry is the only process that can be done collectively like
        # this without complex logic to check we don't have to do things
        # on an instrument-by-instrument basis (e.g. check resolution are
        # the same for imaging, wavelength arrays for spectroscopy etc.).
        self.filters = FilterCollection()
        for inst in instruments:
            if inst.can_do_photometry:
                self.filters += inst.filters

        # Define flags for what we will do
        self._do_lnu_spectra = False
        self._do_fnu_spectra = False
        self._do_luminosities = False
        self._do_fluxes = False
        self._do_lum_lines = False
        self._do_flux_lines = False
        self._do_images_lum = False
        self._do_images_lum_psf = False
        self._do_images_flux = False
        self._do_images_flux_psf = False
        self._do_lnu_data_cubes = False
        self._do_fnu_data_cubes = False
        self._do_spectroscopy = False
        self._do_sfzh = False
        self._do_sfh = False
        self._do_los_optical_depths = False

        # Define the container for all the kwargs needed by each operation
        # This will be a dict of dicts
        self._operation_kwargs = {}

        # Define flags for what we will write out
        self._write_lnu_spectra = False
        self._write_fnu_spectra = False
        self._write_luminosities = False
        self._write_fluxes = False
        self._write_lum_lines = False
        self._write_flux_lines = False
        self._write_images_lum = False
        self._write_images_lum_psf = False
        self._write_images_flux = False
        self._write_images_flux_psf = False
        self._write_lnu_data_cubes = False
        self._write_fnu_data_cubes = False
        self._write_spectroscopy = False
        self._write_sfzh = False
        self._write_sfh = False

        # Define flags to indicate what we have calculated
        self._loaded_galaxies = False
        self._got_lnu_spectra = False
        self._got_fnu_spectra = False
        self._got_luminosities = False
        self._got_fluxes = False
        self._got_lum_lines = False
        self._got_flux_lines = False
        self._got_images_lum = False
        self._got_images_lum_psf = False
        self._got_images_flux = False
        self._got_images_flux_psf = False
        self._got_lnu_data_cubes = False
        self._got_fnu_data_cubes = False
        self._got_spectroscopy = False
        self._got_sfzh = False
        self._got_sfh = False
        self._analysis_complete = False

        # Define containers for any additional analysis functions
        self._analysis_funcs = []
        self._analysis_args = []
        self._analysis_kwargs = []
        self._analysis_results_keys = []
        self._analysis_results = {}

        # It'll be helpful later if we know what line IDs have been requested
        # so we'll store these should get_lines be called.
        self._line_ids = []

        # Initialise containers for all the data we can generate
        self.sfzhs = []
        self.sfhs = []
        self.lnu_spectra = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.fnu_spectra = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.luminosities = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.fluxes = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.lines_lum = {}
        self.line_cont_lum = {}
        self.flux_lines = {}
        self.line_cont_flux = {}
        self.images_lum = {}
        self.images_lum_psf = {}
        self.images_flux = {}
        self.images_flux_psf = {}
        self.lnu_data_cubes = {}
        self.fnu_data_cubes = {}
        self.spectroscopy = {}

        # We'll need to store the timing information to be reported at the
        # end of the pipeline (this will be stored with the operation
        # as the key and the value being the collected time taken).
        self._op_timing = {
            "LOS optical depths": 0.0,
            "SFZH": 0.0,
            "SFH": 0.0,
            "Lnu Spectra": 0.0,
            "Fnu Spectra": 0.0,
            "Luminosities": 0.0,
            "Fluxes": 0.0,
            "Emission Lines Luminosities": 0.0,
            "Emission Lines Fluxes": 0.0,
            "Images Luminosities": 0.0,
            "Images Luminosities PSF": 0.0,
            "Images Fluxes": 0.0,
            "Images Fluxes PSF": 0.0,
            "Lnu Data Cubes": 0.0,
            "Fnu Data Cubes": 0.0,
            "Spectroscopy": 0.0,
            "Analysis": 0.0,
            "Unpacking": 0.0,
        }

        # We'll also report total counts so define a container for those.
        self._op_counts = {
            "LOS optical depths": 0,
            "SFZH": 0,
            "SFH": 0,
            "Lnu Spectra": 0,
            "Fnu Spectra": 0,
            "Luminosities": 0,
            "Fluxes": 0,
            "Emission Lines Luminosities": 0,
            "Emission Lines Fluxes": 0,
            "Images Luminosities": 0,
            "Images Luminosities PSF": 0,
            "Images Fluxes": 0,
            "Images Fluxes PSF": 0,
            "Lnu Data Cubes": 0,
            "Fnu Data Cubes": 0,
            "Spectroscopy": 0,
            "Analysis": 0,
        }

        # Everything that follows is only needed for hybrid parallelism
        # (running with MPI in addition to shared memory parallelism)

        # If we are running with hybrid parallelism, we need to know about
        # the communicator for MPI
        self.comm = comm
        self.using_mpi = comm is not None and comm.Get_size() > 1

        # Get some MPI informaiton if we are using MPI
        if self.using_mpi:
            self.rank = comm.Get_rank()
            self.size = comm.Get_size()
        else:
            self.rank = 0
            self.size = 1

        if self.rank == 0:
            self._say_hello()
            self._report()

        # Attach the writer, we'll assign this later in the write method
        self.io_helper = None

        # Hold back everyone in MPI land until we're all ready to go
        if self.using_mpi:
            comm.Barrier()

    def _say_hello(self):
        """Print a nice welcome."""
        print()
        print("\n".join([" " * 25 + s for s in Art.galaxy.split("\n")]))
        print()

    def _report(self):
        """Print a report containing the Pipeline setup."""
        # Print the MPI setup if we are using MPI
        if self.using_mpi:
            self._print(f"Running with MPI on {self.size} ranks.")

        # Print the shared memory parallelism setup
        if self.nthreads > 1 and self.using_mpi:
            self._print(f"Running with {self.nthreads} threads per rank.")
        elif self.nthreads > 1:
            self._print(f"Running with {self.nthreads} threads.")

        # Print some information about the emission model
        self._print(f"Root emission model: {self.emission_model.label}")
        self._print(
            f"EmissionModel contains {len(self.emission_model._models)} "
            "individual models."
        )
        self._print("EmissionModels split by emitter:")
        label_width = max(
            len("   - galaxy"), len("   - stellar"), len("   - blackhole")
        )
        ngal_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m.emitter == "galaxy"
            ]
        )
        nstar_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m.emitter == "stellar"
            ]
        )
        nbh_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m.emitter == "blackhole"
            ]
        )
        self._print(f"{'   - galaxy:'.ljust(label_width + 2)} {ngal_models}")
        self._print(f"{'   - stellar:'.ljust(label_width + 2)} {nstar_models}")
        self._print(f"{'   - blackhole:'.ljust(label_width + 2)} {nbh_models}")

        self._print("EmissionModels split by operation type:")
        label_width = max(
            len("   - extraction"),
            len("   - combination"),
            len("   - attenuating"),
            len("   - generation"),
        )
        nextract_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_extracting
            ]
        )
        ncombine_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_combining
            ]
        )
        nattenuate_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_transforming
            ]
        )
        ngen_models = len(
            [
                m
                for m in self.emission_model._models.values()
                if m._is_generating or m._is_dust_emitting
            ]
        )
        self._print(
            f"{'   - extraction:'.ljust(label_width + 2)} {nextract_models}"
        )
        self._print(
            f"{'   - combination:'.ljust(label_width + 2)} {ncombine_models}"
        )
        self._print(
            f"{'   - attenuating:'.ljust(label_width + 2)} {nattenuate_models}"
        )
        self._print(
            f"{'   - generation:'.ljust(label_width + 2)} {ngen_models}"
        )

        # Print the number of instruments we have
        self._print(f"Using {len(self.instruments)} instruments.")

        # Print the number of filters we have
        self._print(f"Instruments have {len(self.filters)} filters in total.")

        # Make a breakdown of the instruments
        self._print(
            "Included instruments:",
            ", ".join(list(self.instruments.instruments.keys())),
        )
        self._print("Instruments split by capability:")
        label_width = max(
            len("   - photometry"),
            len("   - spectroscopy"),
            len("   - imaging"),
            len("   - resolved spectroscopy"),
        )
        nphot_inst = len(
            [inst for inst in self.instruments if inst.can_do_photometry]
        )
        nspec_inst = len(
            [inst for inst in self.instruments if inst.can_do_spectroscopy]
        )
        nimg_inst = len(
            [inst for inst in self.instruments if inst.can_do_imaging]
        )
        nresspec_inst = len(
            [
                inst
                for inst in self.instruments
                if inst.can_do_resolved_spectroscopy
            ]
        )
        self._print(
            f"{'   - photometry:'.ljust(label_width + 2)} {nphot_inst}"
        )
        self._print(
            "   - spectroscopy:".ljust(label_width + 2),
            nspec_inst,
        )
        self._print(f"{'   - imaging:'.ljust(label_width + 2)} {nimg_inst}")
        self._print(
            f"{'   - resolved spectroscopy:'.ljust(label_width + 2)}"
            f" {nresspec_inst}"
        )

    def _say_goodbye(self):
        """Print a nice goodbye including timing."""
        elapsed = time.perf_counter() - self._start_time

        # Report in sensible units
        if elapsed < 1:
            elapsed *= 1000
            units = "ms"
        elif elapsed < 60:
            units = "s"
        elif elapsed < 3600:
            elapsed /= 60
            units = "mins"
        else:
            elapsed /= 3600
            units = "hours"

        # Report how blazingly fast we are
        self._print(f"Total synthesis took {elapsed:.3f} {units}.")
        self._print("Goodbye!")

    def _print(self, *args, **kwargs):
        """
        Print a message to the screen with extra information.

        The prints behave differently depending on whether we are using MPI or
        not. We can also set the verbosity level at the Pipeline level which
        will control the verbosity of the print statements.

        Verbosity:
            0: No output beyond hello and goodbye.
            1: Outputs with timings but only on rank 0 (when using MPI).
            2: Outputs with timings on all ranks (when using MPI).

        Args:
            message (str): The message to print.
        """
        # At verbosity 0 we are silent
        if self.verbose == 0:
            return

        # Get the current time code in seconds with 0 padding and 2
        # decimal places
        now = time.perf_counter() - self._start_time
        int_now = str(int(now)).zfill(
            len(str(int(now))) + 1 if now > 9999 else 5
        )
        decimal = str(now).split(".")[-1][:2]
        now_str = f"{int_now}.{decimal}"

        # Create the prefix for the print, theres extra info to output if
        # we are using MPI
        if self.using_mpi:
            # Only print on rank 0 if we are using MPI and have verbosity 1
            if self.verbose == 1 and self.rank != 0:
                return

            prefix = (
                f"[{str(self.rank).zfill(len(str(self.size)) + 1)}]"
                f"[{now_str}]:"
            )

        else:
            prefix = f"[{now_str}]:"

        print(prefix, *args, **kwargs)

    def _took(self, start, message):
        """
        Print a message with the time taken since the start time.

        Args:
            start (float): The start time of the process.
            message (str): The message to print.
        """
        elapsed = time.perf_counter() - start

        # Report in sensible units
        if elapsed < 1:
            elapsed *= 1000
            units = "ms"
        elif elapsed < 60:
            units = "s"
        else:
            elapsed /= 60
            units = "mins"

        # Report how blazingly fast we are
        self._print(f"{message} took {elapsed:.3f} {units}.")

    def add_analysis_func(self, func, result_key, *args, **kwargs):
        """
        Add an analysis function to the Pipeline.

        The provided function will be called on each galaxy in the Pipeline
        once all data has been generated. The function should take a galaxy
        object as the first argument and can take any number of additional
        arguments and keyword arguments.

        The results of the analysis function should be returned. This can
        be a scalar, array, or a dictionary of arbitrary structure. We'll
        store it in a dictionary on the Pipeline object with the key being the
        result_key argument.

        For example:

            ```python
            def my_analysis_func(galaxy, *args, **kwargs):
                return galaxy.some_attribute * 2

            pipeline.add_analysis_func(my_analysis_func, "MyAnalysisResult")
            ```

        Or for a specific component of the galaxy:

            ```python
            def my_analysis_func(galaxy, *args, **kwargs):
                return galaxy.stars.mass.sum()

            pipeline.add_analysis_func(my_analysis_func, "Stars/Mass")
            ```

        Args:
            func (callable):
                The analysis function to add to the Pipeline. This function
                should take a galaxy object as the first argument and can take
                any number of additional arguments and keyword arguments.
            result_key (str):
                The key to use when storing the results of the analysis
                function in the output. This can include slashes to denote
                nesting, e.g. "Gas/Nested/Result".
        """
        # Ensure we have a callable function
        if not callable(func):
            raise exceptions.InconsistentArguments(
                "Analysis function is not a callable function"
                f" (found {type(func)})."
            )

        # Warn the user if theres a name clash, we'll take the new one
        if func.__name__ in self._analysis_funcs:
            warn(
                f"{func.__name__} already exists in the analysis functions. "
                "Overwriting with the passed function."
            )

        # Add the function to the dictionary
        self._analysis_funcs.append(func)
        self._analysis_args.append(args)
        self._analysis_kwargs.append(kwargs)
        self._analysis_results_keys.append(result_key)

        # Warn the user if theres a name clash, we'll take the new one
        if result_key in self._analysis_results:
            raise exceptions.InconsistentArguments(
                f"{result_key} already exists in the analysis results. "
                "Choose a different result_key"
            )
        else:
            self._analysis_results[result_key] = None

        self._print(f"Added analysis function: {result_key}")

    def add_galaxies(self, galaxies):
        """
        Add galaxies to the Pipeline.

        This function will add the provided galaxies to the Pipeline. This is
        useful if you have already loaded the galaxies and want to add them to
        the Pipeline object.

        Args:
            galaxies (list):
                A list of Galaxy objects to add to the Pipeline.
        """
        start = time.perf_counter()

        # Attach the galaxies
        self.galaxies = galaxies
        self.n_galaxies_local = len(galaxies)

        # If we're in MPI land we need sum the local counts across all ranks
        # to get the total number of galaxies
        if self.using_mpi:
            self.n_galaxies = self.comm.allreduce(self.n_galaxies_local)
        else:
            self.n_galaxies = self.n_galaxies_local

        # If we have MPI lets report the balance
        if self.using_mpi:
            self._report_balance()

        # Done!
        self._loaded_galaxies = True
        self._took(start, f"Adding {self.n_galaxies} galaxies")

    def get_los_optical_depths(
        self,
        kernel,
        kernel_threshold=1.0,
        kappa=0.0795,
    ):
        """
        Flag that the Pipeline should compute the LOS optical depths.

        This will signal the Pipeline to compute the LOS optical depths when
        the run method is called.

        LOS optical depths are computed first.

        Note that the LOS calculation requries a galaxy has a gas component and
        either a stellar or black hole components emitting.

        Args:
            kernel (array-like):
                The gas SPH kernel.
            kernel_threshold (float):
                The threshold of the kernel. Default is 1.0.
            kappa (float):
                The dust opacity coefficient in units of Msun / pc**2. Default
                is 0.0795.
        """
        # Store the arguments for the operation
        self._operation_kwargs["get_los_optical_depths"] = {
            "kernel": kernel,
            "kernel_threshold": kernel_threshold,
            "kappa": kappa,
        }

        # Flag that we will compute the LOS optical depths
        self._do_los_optical_depths = True

    def _get_los_optical_depths(self, galaxy):
        """
        Compute the Line of Sight optical depths for all particles.

        This will compute the optical depths based on the line of sight dust
        column density for all non-gas components. We project a ray along the
        z axis (LOS) and any gas kernels it intersects are evaluated at the
        intersection and their contributions to the optical depth is included.

        Args:
            galaxy (Galaxy):
                The galaxy to compute the optical depths for.
        """
        start = time.perf_counter()

        # Compute the optical depths for the present components.
        galaxy.get_stellar_los_tau_v(
            kappa=self._operation_kwargs["get_los_optical_depths"]["kappa"],
            kernel=self._operation_kwargs["get_los_optical_depths"]["kernel"],
            threshold=self._operation_kwargs["get_los_optical_depths"][
                "kernel_threshold"
            ],
            nthreads=self.nthreads,
        )
        galaxy.get_black_hole_los_tau_v(
            kappa=self._operation_kwargs["get_los_optical_depths"]["kappa"],
            kernel=self._operation_kwargs["get_los_optical_depths"]["kernel"],
            threshold=self._operation_kwargs["get_los_optical_depths"][
                "kernel_threshold"
            ],
            nthreads=self.nthreads,
        )

        # Count how many optical depths we have generated (1 per particle) and
        # increment the total counts
        self._op_counts["LOS optical depths"] += (
            galaxy.stars.nstars + galaxy.black_holes.nbh
        )

        # Record the time taken
        self._op_timing["LOS optical depths"] += time.perf_counter() - start

    def get_sfzh(self, log10ages, log10metallicities):
        """
        Flag that the Pipeline should compute the SFZH grid.

        This will signal the Pipeline to compute the SFZH grid when the run
        method is called.

        The SFZH grid is the star formation history grid for each galaxy.

        Args:
            log10ages (array-like):
                The log10 age axis of the SFZH grid.
            log10metallicities (array-like):
                The log10 metallicity axis of the SFZH grid.
        """
        # Store the arguments for the operation
        self._operation_kwargs["get_sfzh"] = {
            "log10ages": log10ages,
            "log10metallicities": log10metallicities,
        }

        # Flag that we will compute the SFZH grid
        self._do_sfzh = True

        # Flag that we will want to write out the SFZH grid (calling the
        # get_sfzh method is considered the the intent to write it out)
        self._write_sfzh = True

    def _get_sfzh(self, galaxy):
        """
        Compute the SFZH grid for each galaxy.

        This is also the integrated weights of each star particle onto the SPS
        grid.

        Args:
            galaxy (Galaxy):
                The galaxy to compute the SFZH grid for.
        """
        start = time.perf_counter()

        # Get the SFZH, skip any without stars.
        # Parametric galaxies have this ready to go so we can skip them
        if getattr(galaxy, "sfzh", None) is not None:
            self.sfzhs.append(galaxy.sfzh)
            return
        elif galaxy.stars is not None and galaxy.stars.nstars > 0:
            galaxy.stars.get_sfzh(
                log10ages=self._operation_kwargs["get_sfzh"]["log10ages"],
                log10metallicities=self._operation_kwargs["get_sfzh"][
                    "log10metallicities"
                ],
                nthreads=self.nthreads,
            )
        else:
            # No stars, no SFZH, store a zeroed grid
            self.sfzhs.append(
                np.zeros(
                    (
                        len(self._operation_kwargs["get_sfzh"]["log10ages"]),
                        len(
                            self._operation_kwargs["get_sfzh"][
                                "log10metallicities"
                            ]
                        ),
                    )
                )
            )

            return

        # Count the number of SFZH grids we have generated
        self._op_counts["SFZH"] += 1

        # Record the time taken
        self._op_timing["SFZH"] += time.perf_counter() - start

    def get_sfh(self, log10ages):
        """
        Flag that the Pipeline should compute the binned SFH.

        This will signal the Pipeline to compute the binned SFH when the run
        method is called.

        The SFH is the binned star formation history based on an arbitrary
        set of age bins define din log10 space.

        Args:
            log10ages (array-like):
                The log10 age axis of the SFH grid.
        """
        # Store the arguments for the operation
        self._operation_kwargs["get_sfh"] = {"log10ages": log10ages}

        # Flag that we will compute the SFH grid
        self._do_sfh = True

        # Flag that we will want to write out the SFH grid (calling the
        # get_sfh method is considered the the intent to write it out)
        self._write_sfh = True

    def _get_sfh(self, galaxy):
        """
        Compute the binned SFH for each galaxy.

        Args:
            galaxy (Galaxy):
                The galaxy to compute the SFH for.
        """
        start = time.perf_counter()

        # Get the SFH, skip any without stars.
        # Parametric galaxies have this ready to go so we can skip them
        if getattr(galaxy, "sfh", None) is not None:
            self.sfhs.append(galaxy.sfh)
            return
        elif galaxy.stars is not None and galaxy.stars.nstars > 0:
            galaxy.stars.get_sfh(
                log10ages=self._operation_kwargs["get_sfh"]["log10ages"],
                nthreads=self.nthreads,
            )
        else:
            # No stars, no SFH, store a zeroed grid
            self.sfhs.append(
                np.zeros(len(self._operation_kwargs["get_sfh"]["log10ages"]))
            )

            return

        # Count the number of SFH grids we have generated
        self._op_counts["SFH"] += 1

        # Record the time taken
        self._op_timing["SFH"] += time.perf_counter() - start

    def get_spectra(self):
        """
        Flag that the Pipeline should compute the rest frame spectra.

        This will signal the Pipeline to compute the rest frame spectral
        luminosity density for each galaxy when the run method is called.

        The spectra are generated based on the EmissionModel and the galaxy
        components.

        Spectral flux densities can be computed with get_observed_spectra.
        """
        # Flag that we will compute the spectra
        self._do_lnu_spectra

        # Flag that we will want to write out the spectra (calling the
        # get_spectra method is considered the intent to write it out)
        self._write_lnu_spectra = True

    def _get_spectra(self, galaxy):
        """
        Generate the spectra for the galaxies based on the EmissionModel.

        Args:
            galaxy (Galaxy):
                The galaxy to generate the spectra for.
        """
        start = time.perf_counter()

        # Get the spectra
        galaxy.get_spectra(self.emission_model, nthreads=self.nthreads)

        # Count the number of spectra we have generated
        self._op_counts["Lnu Spectra"] += count_and_check_dict_recursive(
            galaxy.spectra
        )

        # Record the time taken
        self._op_timing["Lnu Spectra"] += time.perf_counter() - start

    def get_observed_spectra(self, cosmo):
        """
        Flag that the Pipeline should compute the observed spectra.

        This will signal the Pipeline to compute the observed spectral flux
        density for each galaxy when the run method is called.

        The observed spectra are generated based on the rest frame spectra and
        the cosmology.

        Args:
            cosmo (astropy.cosmology.Cosmology):
                The cosmology to use for the observed spectra.
        """
        # Store the cosmology for the operation
        self._operation_kwargs["get_observed_spectra"] = {"cosmo": cosmo}

        # Flag that we will compute the observed spectra
        self._do_fnu_spectra = True

        # To compute the observed spectra we need to have already computed the
        # rest frame spectra
        self._do_lnu_spectra = True

        # Flag that we will want to write out the observed spectra (calling the
        # get_observed_spectra method is considered the intent to write it out)
        self._write_fnu_spectra = True

    def _get_observed_spectra(self, galaxy):
        """
        Compute the observed spectra for each galaxy.

        Args:
            galaxy (Galaxy):
                The galaxy to compute the observed spectra for.
        """
        start = time.perf_counter()

        # Get the observed spectra
        galaxy.get_observed_spectra(
            cosmo=self._operation_kwargs["get_observed_spectra"]["cosmo"],
        )

        # Count the number of observed spectra we have generated
        self._op_counts["Fnu Spectra"] += count_and_check_dict_recursive(
            galaxy.spectra
        )

        # Record the time taken
        self._op_timing["Fnu Spectra"] += time.perf_counter() - start

    def get_photometry_luminosities(self):
        """
        Flag that the Pipeline should compute the photometric luminosities.

        This will signal the Pipeline to compute the photometric luminosities
        for each galaxy when the run method is called.

        The photometric luminosities are generated based on the lnu
        spectra and the instrument filters.
        """
        # Ensure we have filters to compute the photometry
        if len(self.filters) == 0:
            raise exceptions.PipelineNotReady(
                "Cannot generate photometry without instruments with filters! "
                "Add instruments with filters and try again."
            )

        # Flag that we will compute the photometric luminosities
        self._do_luminosities = True

        # To compute the photometric luminosities we need to have already
        # computed the lnu spectra
        self._do_lnu_spectra = True

        # Flag that we will want to write out the photometric luminosities
        # (calling the get_photometry_luminosities method is considered the
        # intent to write it out)
        self._write_luminosities = True

    def _get_photometry_luminosities(self, galaxy):
        """
        Compute the photometric luminosities from the generated spectra.

        Args:
            galaxy (Galaxy):
                The galaxy to compute the photometric luminosities for.
        """
        start = time.perf_counter()

        # Get the photometry.
        galaxy.get_photo_lnu(filters=self.filters, nthreads=self.nthreads)

        # Count the number of photometric luminosities we have generated
        self._op_counts["Luminosities"] += count_and_check_dict_recursive(
            galaxy.luminosities
        )

        # Record the time taken
        self._op_timing["Luminosities"] += time.perf_counter() - start

    def get_photometry_fluxes(self):
        """
        Flag that the Pipeline should compute the photometric fluxes.

        This will signal the Pipeline to compute the photometric fluxes for
        each galaxy when the run method is called.

        The photometric fluxes are generated based on the fnu spectra and the
        instrument filters.
        """
        # Ensure we have filters to compute the photometry
        if len(self.filters) == 0:
            raise exceptions.PipelineNotReady(
                "Cannot generate photometry without instruments with filters! "
                "Add instruments with filters and try again."
            )

        # Flag that we will compute the photometric fluxes
        self._do_fluxes = True

        # To compute the photometric fluxes we need to have already computed
        # the fnu spectra
        self._do_fnu_spectra = True

        # Flag that we will want to write out the photometric fluxes (calling
        # the get_photometry_fluxes method is considered the intent to write it
        # out)
        self._write_fluxes = True

    def _get_photometry_fluxes(self, galaxy):
        """
        Compute the photometric fluxes from the generated spectra.

        Args:
            galaxy (Galaxy):
                The galaxy to compute the photometric fluxes for.
        """
        start = time.perf_counter()

        # Get the photometry.
        galaxy.get_photo_fnu(filters=self.filters, nthreads=self.nthreads)

        # Count the number of photometric fluxes we have generated
        self._op_counts["Fluxes"] += count_and_check_dict_recursive(
            galaxy.fluxes
        )

        # Record the time taken
        self._op_timing["Fluxes"] += time.perf_counter() - start

    def get_lines(self, line_ids):
        """
        Generate the emission lines for the galaxies.

        This function will generate the emission lines for all spectra types
        that were saved when spectra were generated.

        Args:
            line_ids (list):
                The emission line IDs to generate.
        """
        start = time.perf_counter()

        self._print(f"Generating {len(line_ids)} emission lines...")

        # Ensure we are ready
        if not self._loaded_galaxies:
            raise exceptions.PipelineNotReady(
                "Cannot generate emission lines before galaxies are loaded! "
                "Call load_galaxies first."
            )

        # Loop over the galaxies and get the spectra
        for g in self.galaxies:
            g.get_lines(line_ids, self.emission_model, nthreads=self.nthreads)

        # Store the line IDs for later
        self._line_ids = line_ids

        # Unpack the luminosity lines into a dictionary on the Pipeline object
        self.lines_lum = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        self.line_cont_lum = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for spec_type, lines in g.lines.items():
                for line in lines:
                    self.lines_lum["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(line.id, []).append(line.luminosity)
                    self.line_cont_lum["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(line.id, []).append(line.continuum)
            if g.stars is not None:
                for spec_type, lines in g.stars.lines.items():
                    for line in lines:
                        self.lines_lum["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.luminosity)
                        self.line_cont_lum["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.continuum)
            if g.black_holes is not None:
                for spec_type, lines in g.black_holes.lines.items():
                    for line in lines:
                        self.lines_lum["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.luminosity)
                        self.line_cont_lum["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(line.id, []).append(line.continuum)

        # Convert the lists of luminosities to unyt arrays
        for spec_type, lines in self.lines_lum["Galaxy"].items():
            for line_id, lum in lines.items():
                self.lines_lum["Galaxy"][spec_type][line_id] = unyt_array(lum)
                self.line_cont_lum["Galaxy"][spec_type][line_id] = unyt_array(
                    lum
                )
        for spec_type, lines in self.lines_lum["Stars"].items():
            for line_id, lum in lines.items():
                self.lines_lum["Stars"][spec_type][line_id] = unyt_array(lum)
                self.line_cont_lum["Stars"][spec_type][line_id] = unyt_array(
                    lum
                )
        for spec_type, lines in self.lines_lum["BlackHole"].items():
            for line_id, lum in lines.items():
                self.lines_lum["BlackHole"][spec_type][line_id] = unyt_array(
                    lum
                )
                self.line_cont_lum["BlackHole"][spec_type][line_id] = (
                    unyt_array(lum)
                )

        # Count the number of lines we have generated
        n_lines = count_and_check_dict_recursive(self.lines_lum)

        # Done!
        self._got_lum_lines = True

        if self.comm is not None:
            tot_n_lines = self.comm.allreduce(n_lines)
            self._took(
                start,
                f"Getting {n_lines} emission lines ({tot_n_lines} total)",
            )
        else:
            self._took(start, f"Getting {n_lines} emission lines")

    def get_images_luminosity(
        self,
        fov,
        img_type="smoothed",
        kernel=None,
        kernel_threshold=1.0,
    ):
        """
        Compute the luminosity images for the galaxies.

        This function will compute the luminosity images for all spectra types
        that were saved when spectra were generated, in all filters included in
        the Pipeline instruments.

        A PSF and/or noise will be applied if they are available on the
        instrument.

        Args:
            fov (unyt_quantity):
                The field of view of the image with units.
            img_type (str):
                The type of image to generate. Options are 'smoothed' or
                'hist'. Default is 'smoothed'.
            kernel (array-like):
                The kernel to use for smoothing the image. Default is None.
                Required for 'smoothed' images from a particle distribution.
            kernel_threshold (float):
                The threshold of the kernel. Default is 1.0.
        """
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_luminosities:
            raise exceptions.PipelineNotReady(
                "Cannot generate images before luminosities are generated! "
                "Call get_photometry_luminosities first."
            )

        # Loop over instruments and perform any imaging they define
        for inst in self.instruments:
            # Skip if the instrument can't do imaging
            if not inst.can_do_imaging:
                continue

            # Loop over galaxies getting the initial images. We do this on
            # an individual galaxy basis since we can use internal shared
            # memory parallelism to do this
            for g in self.galaxies:
                g.get_images_luminosity(
                    resolution=inst.resolution,
                    fov=fov,
                    emission_model=self.emission_model,
                    img_type=img_type,
                    kernel=kernel,
                    kernel_threshold=kernel_threshold,
                    nthreads=self.nthreads,
                    instrument=inst,
                )

        # Unpack the luminosity images into a dictionary on the Pipeline object
        self.images_lum = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for d in g.images_lnu.values():
                for spec_type, imgs in d.items():
                    for f, img in imgs.items():
                        self.images_lum["Galaxy"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
            if g.stars is not None:
                for d in g.stars.images_lnu.values():
                    for spec_type, imgs in d.items():
                        for f, img in imgs.items():
                            self.images_lum["Stars"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)
            if g.black_holes is not None:
                for d in g.black_holes.images_lnu.values():
                    for spec_type, imgs in d.items():
                        for f, img in imgs.items():
                            self.images_lum["BlackHole"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

        # Convert the lists of images to unyt arrays
        for spec_type, imgs in self.images_lum["Galaxy"].items():
            for f, img in imgs.items():
                self.images_lum["Galaxy"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum["Stars"].items():
            for f, img in imgs.items():
                self.images_lum["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum["BlackHole"].items():
            for f, img in imgs.items():
                self.images_lum["BlackHole"][spec_type][f] = unyt_array(img)

        # Count the number of images we have generated
        n_images = count_and_check_dict_recursive(self.images_lum)

        # Done!
        self._got_images_lum = True

        if self.comm is not None:
            tot_n_images = self.comm.allreduce(n_images)
            self._took(
                start,
                f"Generating {n_images} luminosity images "
                f"({tot_n_images} total)",
            )
        else:
            self._took(start, f"Generating {n_images} luminosity images")

    def apply_psfs_luminosity(self):
        """Apply any instrument PSFs to the luminosity images."""
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_images_lum:
            raise exceptions.PipelineNotReady(
                "Cannot apply PSF to images before images are generated! "
                "Call get_images_luminosity first."
            )

        # Loop over instruments and perform any imaging they define
        self.images_lum_psf = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for inst in self.instruments:
            # Skip if the instrument can't do imaging
            if not inst.can_do_psf_imaging:
                continue

            # Loop over galaxies
            for g in self.galaxies:
                # Ensure we have the PSF dictionary on the galaxy
                if not hasattr(g, "images_psf_lnu"):
                    g.images_psf_lnu = {}

                # Apply PSFs to the galaxy level images
                g.images_psf_lnu.setdefault(inst.label, {})
                for spec_type, imgs in g.images_lnu[inst.label].items():
                    g.images_psf_lnu[inst.label][spec_type] = imgs.apply_psfs(
                        inst.psfs,
                    )

                    # Unpack the image arrays onto the Pipeline object
                    for f, img in g.images_psf_lnu[inst.label][
                        spec_type
                    ].items():
                        self.images_lum_psf["Galaxy"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)

                # Apply PSFs to the stars level images
                if g.stars is not None:
                    if not hasattr(g.stars, "images_psf_lnu"):
                        g.stars.images_psf_lnu = {}
                    g.stars.images_psf_lnu.setdefault(inst.label, {})
                    for spec_type, imgs in g.stars.images_lnu[
                        inst.label
                    ].items():
                        g.stars.images_psf_lnu[inst.label][spec_type] = (
                            imgs.apply_psfs(
                                inst.psfs,
                            )
                        )

                        # Unpack the image arrays onto the Pipeline object
                        for f, img in g.stars.images_psf_lnu[inst.label][
                            spec_type
                        ].items():
                            self.images_lum_psf["Stars"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

                # Apply PSFs to the black hole level images
                if g.black_holes is not None:
                    if not hasattr(g.black_holes, "images_psf_lnu"):
                        g.black_holes.images_psf_lnu = {}
                    g.black_holes.images_psf_lnu.setdefault(inst.label, {})
                    for spec_type, imgs in g.black_holes.images_lnu[
                        inst.label
                    ].items():
                        g.black_holes.images_psf_lnu[inst.label][spec_type] = (
                            imgs.apply_psfs(
                                inst.psfs,
                            )
                        )

                        # Unpack the image arrays onto the Pipeline object
                        for f, img in g.black_holes.images_psf_lnu[inst.label][
                            spec_type
                        ].items():
                            self.images_lum_psf["BlackHole"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

        # Convert the lists of images to unyt arrays
        for spec_type, imgs in self.images_lum_psf["Galaxy"].items():
            for f, img in imgs.items():
                self.images_lum_psf["Galaxy"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum_psf["Stars"].items():
            for f, img in imgs.items():
                self.images_lum_psf["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_lum_psf["BlackHole"].items():
            for f, img in imgs.items():
                self.images_lum_psf["BlackHole"][spec_type][f] = unyt_array(
                    img
                )

        # Count the number of images we have generated
        n_images = count_and_check_dict_recursive(self.images_lum_psf)

        # Done!
        self._got_images_lum_psf = True

        if self.comm is not None:
            tot_n_images = self.comm.allreduce(n_images)
            self._took(
                start,
                f"Applying PSFs to {n_images} luminosity images "
                f"({tot_n_images} total)",
            )
        else:
            self._took(start, f"Applying PSFs to {n_images} luminosity images")

    def get_images_flux(
        self,
        fov,
        img_type="smoothed",
        kernel=None,
        kernel_threshold=1.0,
    ):
        """
        Compute the flux images for the galaxies.

        This function will compute the flux images for all spectra types that
        were saved when spectra were generated, in all filters included in the
        Pipeline instruments.

        A PSF and/or noise will be applied if they are available on the
        instrument.

        Args:
            fov (unyt_quantity):
                The field of view of the image with units.
            img_type (str):
                The type of image to generate. Options are 'smoothed' or
                'hist'. Default is 'smoothed'.
            kernel (array-like):
                The kernel to use for smoothing the image. Default is None.
                Required for 'smoothed' images from a particle distribution.
            kernel_threshold (float):
                The threshold of the kernel. Default is 1.0.
        """
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_fluxes:
            raise exceptions.PipelineNotReady(
                "Cannot generate images before fluxes are generated! "
                "Call get_photometry_fluxes first."
            )

        # Loop over instruments and perform any imaging they define
        for inst in self.instruments:
            # Skip if the instrument can't do imaging
            if not inst.can_do_imaging:
                continue

            # Loop over galaxies getting the initial images. We do this on
            # an individual galaxy basis since we can use internal shared
            # memory parallelism to do this
            for g in self.galaxies:
                g.get_images_flux(
                    resolution=inst.resolution,
                    fov=fov,
                    emission_model=self.emission_model,
                    img_type=img_type,
                    kernel=kernel,
                    kernel_threshold=kernel_threshold,
                    nthreads=self.nthreads,
                    instrument=inst,
                )

        # Unpack the flux images into a dictionary on the Pipeline object
        self.images_flux = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for g in self.galaxies:
            for d in g.images_fnu.values():
                for spec_type, imgs in d.items():
                    for f, img in imgs.items():
                        self.images_flux["Galaxy"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)
            if g.stars is not None:
                for d in g.stars.images_fnu.values():
                    for spec_type, imgs in d.items():
                        for f, img in imgs.items():
                            self.images_flux["Stars"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)
            if g.black_holes is not None:
                for d in g.black_holes.images_fnu.values():
                    for spec_type, imgs in d.items():
                        for f, img in imgs.items():
                            self.images_flux["BlackHole"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

        # Convert the lists of images to unyt arrays
        for spec_type, imgs in self.images_flux["Galaxy"].items():
            for f, img in imgs.items():
                self.images_flux["Galaxy"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux["Stars"].items():
            for f, img in imgs.items():
                self.images_flux["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux["BlackHole"].items():
            for f, img in imgs.items():
                self.images_flux["BlackHole"][spec_type][f] = unyt_array(img)

        # Count the number of images we have generated
        n_images = count_and_check_dict_recursive(self.images_flux)

        # Done!
        self._got_images_flux = True

        if self.comm is not None:
            tot_n_images = self.comm.allreduce(n_images)
            self._took(
                start,
                f"Generating {n_images} flux images ({tot_n_images} total)",
            )
        else:
            self._took(start, f"Generating {n_images} flux images")

    def apply_psfs_flux(self):
        """Apply any instrument PSFs to the flux images."""
        start = time.perf_counter()

        # Ensure we are ready
        if not self._got_images_flux:
            raise exceptions.PipelineNotReady(
                "Cannot apply PSF to images before images are generated! "
                "Call get_images_flux first."
            )

        # Loop over instruments and perform any imaging they define
        self.images_flux_psf = {"Galaxy": {}, "Stars": {}, "BlackHole": {}}
        for inst in self.instruments:
            # Skip if the instrument can't do imaging
            if not inst.can_do_psf_imaging:
                continue

            # Loop over galaxies
            for g in self.galaxies:
                # Ensure we have the PSF dictionary on the galaxy
                if not hasattr(g, "images_psf_fnu"):
                    g.images_psf_fnu = {}

                # Apply PSFs to the galaxy level images
                g.images_psf_fnu.setdefault(inst.label, {})
                for spec_type, imgs in g.images_fnu[inst.label].items():
                    g.images_psf_fnu[inst.label][spec_type] = imgs.apply_psfs(
                        inst.psfs,
                    )

                    # Unpack the image arrays onto the Pipeline object
                    for f, img in g.images_psf_fnu[inst.label][
                        spec_type
                    ].items():
                        self.images_flux_psf["Galaxy"].setdefault(
                            spec_type, {}
                        ).setdefault(f, []).append(img.arr * img.units)

                # Apply PSFs to the stars level images
                if g.stars is not None:
                    if not hasattr(g.stars, "images_psf_fnu"):
                        g.stars.images_psf_fnu = {}
                    g.stars.images_psf_fnu.setdefault(inst.label, {})
                    for spec_type, imgs in g.stars.images_fnu[
                        inst.label
                    ].items():
                        g.stars.images_psf_fnu[inst.label][spec_type] = (
                            imgs.apply_psfs(
                                inst.psfs,
                            )
                        )

                        # Unpack the image arrays onto the Pipeline object
                        for f, img in g.stars.images_psf_fnu[inst.label][
                            spec_type
                        ].items():
                            self.images_flux_psf["Stars"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

                # Apply PSFs to the black hole level images
                if g.black_holes is not None:
                    if not hasattr(g.black_holes, "images_psf_fnu"):
                        g.black_holes.images_psf_fnu = {}
                    g.black_holes.images_psf_fnu.setdefault(inst.label, {})
                    for spec_type, imgs in g.black_holes.images_fnu[
                        inst.label
                    ].items():
                        g.black_holes.images_psf_fnu[inst.label][spec_type] = (
                            imgs.apply_psfs(
                                inst.psfs,
                            )
                        )

                        # Unpack the image arrays onto the Pipeline object
                        for f, img in g.black_holes.images_psf_fnu[inst.label][
                            spec_type
                        ].items():
                            self.images_flux_psf["BlackHole"].setdefault(
                                spec_type, {}
                            ).setdefault(f, []).append(img.arr * img.units)

        # Convert the lists of images to unyt arrays
        for spec_type, imgs in self.images_flux_psf["Galaxy"].items():
            for f, img in imgs.items():
                self.images_flux_psf["Galaxy"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux_psf["Stars"].items():
            for f, img in imgs.items():
                self.images_flux_psf["Stars"][spec_type][f] = unyt_array(img)
        for spec_type, imgs in self.images_flux_psf["BlackHole"].items():
            for f, img in imgs.items():
                self.images_flux_psf["BlackHole"][spec_type][f] = unyt_array(
                    img
                )

        # Count the number of images we have generated
        n_images = count_and_check_dict_recursive(self.images_flux_psf)

        # Done!
        self._got_images_flux_psf = True

        if self.comm is not None:
            tot_n_images = self.comm.allreduce(n_images)
            self._took(
                start,
                f"Applying PSFs to {n_images} flux images "
                f"({tot_n_images} total)",
            )
        else:
            self._took(start, f"Applying PSFs to {n_images} flux images")

    def get_data_cubes_lnu(self):
        """Compute the spectral luminosity density data cubes."""
        start = time.perf_counter()
        raise exceptions.NotImplemented(
            "Data cubes are not yet implemented in Pipelines."
        )

        # Done!
        self._got_lnu_data_cubes = True
        self._took(start, "Getting lnu data cubes")

    def get_data_cubes_fnu(self):
        """Compute the Spectral flux density data cubes."""
        start = time.perf_counter()
        raise exceptions.NotImplemented(
            "Data cubes are not yet implemented in Pipelines."
        )

        # Done!
        self._got_fnu_data_cubes = True
        self._took(start, "Getting fnu data cubes")

    def _run_extra_analysis(self):
        """
        Call any user provided analysis functions.

        We will call this just before writing out all data. This ensures that
        all data generated by the pipeline exists before performing the user
        calculations. This is important since the user may want to use the
        data generated by the pipeline in their analysis functions.
        """
        start = time.perf_counter()

        # Nothing to do if we have no analysis functions
        if len(self._analysis_funcs) == 0:
            return

        # Loop over the analysis functions and run them on each individual
        # galaxy. We can do this with a threadpool if we have multiple threads.
        for func, args, kwargs, key in zip(
            self._analysis_funcs,
            self._analysis_args,
            self._analysis_kwargs,
            self._analysis_results_keys,
        ):
            # Run the analysis function on each galaxy
            try:
                func_start = time.perf_counter()
                res = []
                for g in self.galaxies:
                    res.append(func(g, *args, **kwargs))
            except Exception as e:
                self._print(
                    "Error running extra analysis function" f" {key}: {e}"
                )

            # Store the results and combine them if necessary
            try:
                # Check we actually have some results
                if len(res) == 0:
                    self._print(
                        f"Extra analysis function {key} returned no results"
                    )
                    continue

                # Ensure the data of all results is in the same format.
                types = set([type(r) for r in res])
                if len(types) > 1:
                    raise exceptions.BadResult(
                        "All results from extra analysis functions must be "
                        f"of the same type. Got: {set(types)}"
                    )

                # If we have a list of dictionaries then we need to combine
                # them into a single dictionary. Otherwise we can just store
                # the list of results as a unyt array.
                if isinstance(res[0], dict):
                    combined_data = combine_list_of_dicts(res)
                else:
                    combined_data = unyt_array(res)

                # Ensure we have data after combining
                if len(combined_data) == 0:
                    self._print(
                        f"Extra analysis function {key} returned no data"
                    )
                    continue

                # Store the data
                self._analysis_results[key] = combined_data

                self._took(func_start, f"{key} extra analysis")

            except Exception as e:
                self._print(
                    "Error storing extra analysis results" f" {key}: {e}"
                )

        # Count the number of extra analysis results we have generated
        n_extra_analysis = count_and_check_dict_recursive(
            self._analysis_results
        )

        # Done!
        if self.comm is not None:
            tot_n_extra_analysis = self.comm.allreduce(n_extra_analysis)
            self._took(
                start,
                f"Extra analysis (producing {n_extra_analysis} results) "
                f"({tot_n_extra_analysis} total)",
            )
        else:
            self._took(
                start, f"Extra analysis (producing {n_extra_analysis} results)"
            )

    def _unpack_results(self, galaxy):
        """
        Unpack the results from the galaxy into the pipeline.

        This will extract all the calculated data from the galaxy and store it
        on the pipeline for writing out. All data not flagged to be saved will
        be cleared when the galaxy is garbage collected.

        Args:
            galaxy (Galaxy):
                The galaxy to unpack the results from.
        """
        start = time.perf_counter()

        # Do we need to unpack the SFZH?
        if self._write_sfzh:
            self.sfzhs.append(galaxy.sfzh)

        # Do we need to unpack the SFH?
        if self._write_sfh:
            self.sfhs.append(galaxy.sfh)

        # Do we need to unpack the lnu spectra?
        if self._write_lnu_spectra:
            for spec_type, spec in galaxy.spectra.items():
                self.lnu_spectra["Galaxy"].setdefault(spec_type, []).append(
                    spec.lnu
                )
            if galaxy.stars is not None:
                for spec_type, spec in galaxy.stars.spectra.items():
                    self.lnu_spectra["Stars"].setdefault(spec_type, []).append(
                        spec.lnu
                    )
            if galaxy.black_holes is not None:
                for spec_type, spec in galaxy.black_holes.spectra.items():
                    self.lnu_spectra["BlackHole"].setdefault(
                        spec_type, []
                    ).append(spec.lnu)

        # Do we need to unpack the fnu spectra?
        if self._write_fnu_spectra:
            for spec_type, spec in galaxy.spectra.items():
                self.fnu_spectra["Galaxy"].setdefault(spec_type, []).append(
                    spec.fnu
                )
            if galaxy.stars is not None:
                for spec_type, spec in galaxy.stars.spectra.items():
                    self.fnu_spectra["Stars"].setdefault(spec_type, []).append(
                        spec.fnu
                    )
            if galaxy.black_holes is not None:
                for spec_type, spec in galaxy.black_holes.spectra.items():
                    self.fnu_spectra["BlackHole"].setdefault(
                        spec_type, []
                    ).append(spec.fnu)

        # Do we need to unpack the photometric luminosities?
        if self._write_photometry_luminosities:
            for spec_type, phot in galaxy.photo_lnu.items():
                for filt, lnu in phot.items():
                    self.luminosities["Galaxy"].setdefault(
                        spec_type, {}
                    ).setdefault(filt, []).append(lnu)
            if galaxy.stars is not None:
                for spec_type, phot in galaxy.stars.photo_lnu.items():
                    for filt, lnu in phot.items():
                        self.luminosities["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(lnu)
            if galaxy.black_holes is not None:
                for spec_type, phot in galaxy.black_holes.photo_lnu.items():
                    for filt, lnu in phot.items():
                        self.luminosities["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(lnu)

        # Do we need to unpack the photometric fluxes?
        if self._write_photometry_fluxes:
            for spec_type, phot in galaxy.photo_fnu.items():
                for filt, fnu in phot.items():
                    self.fluxes["Galaxy"].setdefault(spec_type, {}).setdefault(
                        filt, []
                    ).append(fnu)
            if galaxy.stars is not None:
                for spec_type, phot in galaxy.stars.photo_fnu.items():
                    for filt, fnu in phot.items():
                        self.fluxes["Stars"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(fnu)
            if galaxy.black_holes is not None:
                for spec_type, phot in galaxy.black_holes.photo_fnu.items():
                    for filt, fnu in phot.items():
                        self.fluxes["BlackHole"].setdefault(
                            spec_type, {}
                        ).setdefault(filt, []).append(fnu)

        # Done!
        self._op_timing["Unpacking results"] += time.perf_counter() - start

    def run(self):
        """
        Run the pipeline.

        This will churn throuh the attached galaxies generating all the data
        requested using the get_* methods.

        Only data flagged for saving will be held in memory with all other data
        cleared out.

        Once the pipeline has run, the data can be written out to a file using
        the write method.

        Note that as we loop over galaxies they will be removed from the
        pipeline to free up memory. This means that once the pipeline has run
        the galaxies will no longer be accessible from the pipeline object.

        Raises:
            PipelineNotReady:
                If the pipeline is not ready to a specific operation.
        """
        # Ensure we are ready
        if not self._loaded_galaxies:
            raise exceptions.PipelineNotReady(
                "Cannot generate spectra before galaxies are loaded! "
                "Call load_galaxies first."
            )

        # Ensure we have at least one operation signalled
        signals = [
            self._do_los_optical_depths,
            self._do_sfzh,
            self._do_lnu_spectra,
            self._do_fnu_spectra,
        ]
        if not any(signals):
            raise exceptions.PipelineNotReady(
                "Cannot run pipeline without any operations signalled! "
                "Use the get_* methods to signal operations."
            )

        # Loop over galaxie and compute what has been requested using get_*
        # signalling methods
        while len(self.galaxies) > 0:
            # Pop the first galaxy from the list
            gal = self.galaxies.pop(0)

            # Are we generating LOS optical depths?
            if self._do_los_optical_depths:
                self._get_los_optical_depths(gal)

            # Are we generating SFZHs?
            if self._do_sfzh:
                self._get_sfzh(gal)

            # Are we generating SFHs?
            if self._do_sfh:
                self._get_sfh(gal)

            # Are we generating lnu spectra?
            if self._do_lnu_spectra:
                self._get_spectra(gal)

            # Are we generating fnu spectra?
            if self._do_fnu_spectra:
                self._get_observed_spectra(gal)

            # Are we generating photometric luminosities?
            if self._do_photometry_luminosities:
                self._get_photometry_luminosities(gal)

            # Are we generating photometric fluxes?
            if self._do_photometry_fluxes:
                self._get_photometry_fluxes(gal)

            # Ok, we're done with this galaxy so we can unpack the results
            self._unpack_results(gal)

            # Now we can remove the galaxy to free up memory
            del gal

    def _clean_outputs(self):
        """
        Clean up the lists attached to the pipeline.

        This prepares the results for writing out to a file.
        """
        # Convert the lists of SFZHs to unyt arrays
        self.sfzhs = unyt_array(self.sfzhs)

        # Convert the lists of SFHs to unyt arrays
        self.sfhs = unyt_array(self.sfhs)

        # Convert the lists of spectra to unyt arrays
        for spec_type, spec in self.lnu_spectra["Galaxy"].items():
            self.lnu_spectra["Galaxy"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.lnu_spectra["Stars"].items():
            self.lnu_spectra["Stars"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.lnu_spectra["BlackHole"].items():
            self.lnu_spectra["BlackHole"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.fnu_spectra["Galaxy"].items():
            self.fnu_spectra[spec_type] = unyt_array(spec)
        for spec_type, spec in self.fnu_spectra["Stars"].items():
            self.fnu_spectra["Stars"][spec_type] = unyt_array(spec)
        for spec_type, spec in self.fnu_spectra["BlackHole"].items():
            self.fnu_spectra["BlackHole"][spec_type] = unyt_array(spec)

        # Convert the lists of luminosities to unyt arrays
        for spec_type, phot in self.luminosities["Galaxy"].items():
            for filt, lnu in phot.items():
                self.luminosities["Galaxy"][spec_type][filt] = unyt_array(lnu)
        for spec_type, phot in self.luminosities["Stars"].items():
            for filt, lnu in phot.items():
                self.luminosities["Stars"][spec_type][filt] = unyt_array(lnu)
        for spec_type, phot in self.luminosities["BlackHole"].items():
            for filt, lnu in phot.items():
                self.luminosities["BlackHole"][spec_type][filt] = unyt_array(
                    lnu
                )

        # Convert the lists of fluxes to unyt arrays
        for spec_type, phot in self.fluxes["Galaxy"].items():
            for filt, fnu in phot.items():
                self.fluxes["Galaxy"][spec_type][filt] = unyt_array(fnu)
        for spec_type, phot in self.fluxes["Stars"].items():
            for filt, fnu in phot.items():
                self.fluxes["Stars"][spec_type][filt] = unyt_array(fnu)
        for spec_type, phot in self.fluxes["BlackHole"].items():
            for filt, fnu in phot.items():
                self.fluxes["BlackHole"][spec_type][filt] = unyt_array(fnu)

    def write(
        self,
        outpath,
        verbose=None,
        output_lnu=True,
        output_fnu=True,
        output_lum=True,
        output_flux=True,
        output_lines=True,
        output_images_lnu=True,
        output_images_fnu=True,
        output_images_lnu_psf=True,
        output_images_fnu_psf=True,
    ):
        """
        Write what we have produced to a HDF5 file.

        By default everything that has been calculated will be written out. If
        you only want a subset of the data then set the appropriate flags to
        False.

        Args:
            outpath (str):
                The path to the HDF5 file to write.
            verbose (bool, optional):
                If set, override the Pipeline verbose setting.
            output_lnu (bool, optional):
                If True, write out the spectral luminosity densities.
                Default is True.
            output_fnu (bool, optional):
                If True, write out the spectral flux densities.
                Default is True.
            output_lum (bool, optional):
                If True, write out the photometric luminosities.
                Default is True.
            output_flux (bool, optional):
                If True, write out the photometric fluxes.
                Default is True.
            output_lines (bool, optional):
                If True, write out the emission line luminosities.
                Default is True.
            output_images_lnu (bool, optional):
                If True, write out the luminosity images.
                Default is True.
            output_images_fnu (bool, optional):
                If True, write out the flux images.
                Default is True.
            output_images_lnu_psf (bool, optional):
                If True, write out the luminosity images with PSFs applied.
                Default is True.
            output_images_fnu_psf (bool, optional):
                If True, write out the flux images with PSFs applied.
                Default is True.
        """
        # We're done with everything so we know we'll have what is needed for
        # any extra analysis asked for by the user. We'll run these now.
        self._run_extra_analysis()

        write_start = time.perf_counter()

        # First port of call, clean up the data
        self._clean_outputs()

        # Get an instance of the HDF5 file writer
        self.io_helper = PipelineIO(
            outpath,
            self.comm,
            self.n_galaxies_local,
            self._start_time,
            verbose if verbose is not None else self.verbose,
        )

        # Write out the metadata
        self.io_helper.create_file_with_metadata(
            self.instruments, self.emission_model
        )

        # In MPI land we need to collect together the galaxy counts on each
        # rank to make the indices for each rank consistent
        if self.using_mpi:
            galaxy_counts = self.comm.allgather(self.n_galaxies_local)
            galaxy_stars = np.cumsum(galaxy_counts) - galaxy_counts
            galaxy_ends = np.cumsum(galaxy_counts) - 1
            galaxy_indices = np.arange(
                galaxy_stars[self.rank],
                galaxy_ends[self.rank] + 1,
            )
        else:
            galaxy_indices = None

        # Write spectral luminosity densities
        if self._got_lnu_spectra and output_lnu:
            self.io_helper.write_data(
                self.lnu_spectra["Galaxy"],
                "Galaxies/Spectra/SpectralLuminosityDensities",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.lnu_spectra["Stars"],
                "Galaxies/Stars/Spectra/SpectralLuminosityDensities",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.lnu_spectra["BlackHole"],
                "Galaxies/BlackHoles/Spectra/SpectralLuminosityDensities",
                galaxy_indices,
            )

        # Write spectral flux densities
        if self._got_fnu_spectra and output_fnu:
            self.io_helper.write_data(
                self.fnu_spectra["Galaxy"],
                "Galaxies/Spectra/SpectralFluxDensities",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.fnu_spectra["Stars"],
                "Galaxies/Stars/Spectra/SpectralFluxDensities",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.fnu_spectra["BlackHole"],
                "Galaxies/BlackHoles/Photometry/SpectralFluxDensities",
                galaxy_indices,
            )

        # Write photometric luminosities
        if self._got_luminosities and output_lum:
            self.io_helper.write_data(
                self.luminosities["Galaxy"],
                "Galaxies/Photometry/Luminosities",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.luminosities["Stars"],
                "Galaxies/Stars/Photometry/Luminosities",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.luminosities["BlackHole"],
                "Galaxies/BlackHoles/Photometry/Luminosities",
                galaxy_indices,
            )

        # Write photometric fluxes
        if self._got_fluxes and output_flux:
            self.io_helper.write_data(
                self.fluxes["Galaxy"],
                "Galaxies/Photometry/Fluxes",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.fluxes["Stars"],
                "Galaxies/Stars/Photometry/Fluxes",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.fluxes["BlackHole"],
                "Galaxies/BlackHoles/Photometry/Fluxes",
                galaxy_indices,
            )

        # Write emission line luminosities
        if self._got_lum_lines and output_lines:
            self.io_helper.write_data(
                self.lines_lum["Galaxy"],
                "Galaxies/Lines/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.lines_lum["Stars"],
                "Galaxies/Stars/Lines/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.lines_lum["BlackHole"],
                "Galaxies/BlackHoles/Lines/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.line_cont_lum["Galaxy"],
                "Galaxies/Lines/Continuum",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.line_cont_lum["Stars"],
                "Galaxies/Stars/Lines/Continuum",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.line_cont_lum["BlackHole"],
                "Galaxies/BlackHoles/Lines/Continuum",
                galaxy_indices,
            )

        # Write luminosity images
        if self._got_images_lum and output_images_lnu:
            self.io_helper.write_data(
                self.images_lum["Galaxy"],
                "Galaxies/Images/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_lum["Stars"],
                "Galaxies/Stars/Images/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_lum["BlackHole"],
                "Galaxies/BlackHoles/Images/Luminosity",
                galaxy_indices,
            )

        # Write PSF luminosity images
        if self._got_images_lum_psf and output_images_lnu_psf:
            self.io_helper.write_data(
                self.images_lum_psf["Galaxy"],
                "Galaxies/PSFImages/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_lum_psf["Stars"],
                "Galaxies/Stars/PSFImages/Luminosity",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_lum_psf["BlackHole"],
                "Galaxies/BlackHoles/PSFImages/Luminosity",
                galaxy_indices,
            )

        # Write flux images (again these are heavy so we'll collect them
        # separately)
        if self._got_images_flux and output_images_fnu:
            self.io_helper.write_data(
                self.images_flux["Galaxy"],
                "Galaxies/Images/Flux",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_flux["Stars"],
                "Galaxies/Stars/Images/Flux",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_flux["BlackHole"],
                "Galaxies/BlackHoles/Images/Flux",
                galaxy_indices,
            )

        # Write PSF flux images
        if self._got_images_flux_psf and output_images_fnu_psf:
            self.io_helper.write_data(
                self.images_flux_psf["Galaxy"],
                "Galaxies/PSFImages/Flux",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_flux_psf["Stars"],
                "Galaxies/Stars/PSFImages/Flux",
                galaxy_indices,
            )
            self.io_helper.write_data(
                self.images_flux_psf["BlackHole"],
                "Galaxies/BlackHoles/PSFImages/Flux",
                galaxy_indices,
            )

        # Write out the extra analysis results
        for key, res in self._analysis_results.items():
            self.io_helper.write_data(
                res,
                f"Galaxies/{key}",
                galaxy_indices,
            )

        self._took(write_start, "Writing data")

        # Totally done!
        self._say_goodbye()

    def combine_files(self):
        """
        Combine inidividual rank files into a single file.

        Only applicable to MPI runs.

        This will create a physical file on disk with all the data copied from
        the inidivdual rank files. The rank files themselves will be deleted.
        Once all data has been copied.

        This method is cleaner but has the potential to be very slow.
        """
        start = time.perf_counter()

        # Nothing to do if we're not using MPI
        if not self.using_mpi:
            self._print("Not using MPI, nothing to combine.")
            return

        # Ensure we have written the data
        if self.io_helper is None:
            raise exceptions.PipelineNotReady(
                "Cannot combine files before writing data! Call write first."
            )

        # Nothing to do if we're using collective I/O
        if self.io_helper.is_collective:
            self._print("Using collective I/O, nothing to combine.")
            return

        # Combine the files
        self.io_helper.combine_rank_files()

        self._took(start, "Combining files")

    def combine_files_virtual(self):
        """
        Combine inidividual rank files into a single virtual file.

        Only applicable to MPI runs.

        This will create a file where all the data is accessible but not
        physically copied. This is much faster than making a true copy but
        requires each individual rank file remains accessible.
        """
        start = time.perf_counter()

        # Nothing to do if we're not using MPI
        if not self.using_mpi:
            self._print("Not using MPI, nothing to combine.")
            return

        # Ensure we have written the data
        if self.io_helper is None:
            raise exceptions.PipelineNotReady(
                "Cannot combine files before writing data! Call write first."
            )

        # Nothing to do if we're using collective I/O
        if self.io_helper.is_collective:
            self._print("Using collective I/O, nothing to combine.")
            return

        # Combine the files
        self.io_helper.combine_rank_files_virtual()

        self._took(start, "Combining files (virtual)")

    def repartition_galaxies(self, galaxy_weights=None, random_seed=42):
        """Given the galaxies repartition them across the ranks."""
        raise NotImplementedError("Repartitioning is not yet implemented.")

    def _report_balance(self):
        """
        Report the balance of galaxies across the ranks.

        This function will print out a nice horizontal bar graph showing the
        distribution of galaxies across the ranks.
        """
        # Communicate local counts to rank 0
        counts = self.comm.gather(self.n_galaxies_local)

        # Produce a nice horizontal bar graph to show the
        # distribution of galaxies across the ranks. This only needs printing
        # on rank 0 regardless of verbosity.
        if self.rank == 0:
            self._print("Galaxy balance across ranks:")
            # Find the maximum list length for scaling
            max_count = max(counts)

            for rank, count in enumerate(counts):
                # Calculate the length of the bar based on the relative size
                bar_length = int((count / max_count) * 50)

                # Create the bar and append the list length in brackets
                bar = "#" * bar_length
                self._print(
                    f"Rank {str(rank).zfill(len(str(self.size)) + 1)} - "
                    f"{bar} ({count})"
                )
